---
title: CS294-131 Spring 2019
layout: default
---

## Instructors

<div class="instructor">
  <a href="https://people.eecs.berkeley.edu/~trevor/">
  <div class="instructorphoto"><img src="trevordarrell.jpg"></div>
  <div>Trevor Darrell</div>
  </a>
</div>
<div class="instructor">
  <a href="https://people.eecs.berkeley.edu/~dawnsong/">
  <div class="instructorphoto"><img src="dawnsong.jpg" height="120" width="140"></div>
  <div>Dawn Song
  </div>
  </a>

</div>
<div class="instructor">
  <a href="https://cs.stanford.edu/~jsteinhardt/">
  <div class="instructorphoto"><img src="Jacobsteinhardt.png" height="120" width="140"></div>
  <div>Jacob Steinhardt
  </div>
  </a>

</div>
## Teaching Assistants

<div class="instructor">
  <a href="https://people.eecs.berkeley.edu/~sazadi/">
  <div class="instructorphoto"><img src="Samaneh-Azadi.jpg" height="120" width="140"></div>
  <div>Samaneh Azadi
  </div>
  </a>
</div>

## Office Hours

Samaneh Azadi: By appointment

## Lectures

**Time**: Monday 4:00-5:30pm

**Location**: Soda 306

## Piazza

Course announcements will be announced through Piazza. If you are in the class,
[<a href="https://piazza.com/class/joy4z1cunad9h"> **sign up on Piazza**])(https://piazza.com/class/joy4z1cunad9h).

## Syllabus
<table style="table-layout: fixed; font-size: 88%;">
  <thead>
    <tr>
      <th style="width: 5%;">Date</th>
      <th style="width: 17%;">Speaker</th>
      <th style="width: 10%;">Topic</th>
      <th style="width: 40%;">Readings</th>
      <th style="width: 15%;">Talk</th>
      <th style="width: 15%;">Deadlines</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> 1/28 </td>
      <td>Jacob Steinhardt</td>
      <td>  course overview/security </td>
      <td>
      <ul>
      </ul>
      </td>
      <td> <li> <a href="https://drive.google.com/file/d/1xquUpTv2tTBMGwrEwz--ok5e4pt4HmJS/view?usp=sharing">Slides</a> </li>
      <li> <a href="https://www.youtube.com/watch?v=H8st9GuC5SI&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8">Video</a> </li></td>
      <td></td>
    </tr>
    <tr>
        <td> 2/4 </td>
        <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#nicolas-paprenot-a-marauder's-map-of-security-and-privacy-in-machine-learning">Nicolas Papernot</a> </td>
        <td>security/privacy </td>
        <td><u>Main Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1705.07204">Ensemble adversarial training</a></li>
        <li><a href="https://arxiv.org/abs/1802.08908">Scalable private learning with PATE</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1703.04730">Understanding black-box predictions via influence functions</a></li>
        <li><a href="https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf">Membership inference attacks against machine learning models</a></li>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td> 2/11 </td>
        <td> <a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#justin-gilmer-adversarial-examples-are-a-natural-consequence-of-test-error-in-noise">Justin Gilmer </a> </td>
        <td>adversarial examples </td>
        <td><u>Main Reading:</u>
        <ul><li><a href="https://arxiv.org/pdf/1807.01697.pdf">Benchmarking Neural Network Robustness To Common Corruptions And Perturbations</a></li>
        <li><a href="https://arxiv.org/pdf/1901.10513.pdf">Adversarial Examples Are a Natural Consequence of Test Error in Noise</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1807.06732">
        Motivating the Rules of the Game for Adversarial Example Research</a></li>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td> 2/18 </td>
        <td>Academic Holiday</td>
        <td> - </td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td> 2/25 </td>
        <td>Stefan Wager</td>
        <td>causality </td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
    <td> 3/4 </td>
    <td>Guillaume Bouchard </td>
    <td>fake news defense </td>
    <td>
    <ul>
    </ul>
    </td>
    <td></td>
    <td></td>
    </tr>

    <tr>
        <td> 3/11 </td>
        <td>Zachary Lipton</td>
        <td> explainability</td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
    <td> 3/18 </td>
    <td> Dustin Tran </td>
    <td> Bayesian Deep learning</td>
    <td>
    <ul>
    </ul>
    </td>
    <td></td>
    <td></td>
    </tr>

    <tr>
        <td> 3/25 </td>
        <td>Spring Break</td>
        <td> -</td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td> 4/1 </td>
        <td>Alex Alemi, Ian Fischer </td>
        <td>information theory </td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td> 4/8 </td>
        <td>Been Kim</td>
        <td> explainability </td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
    <td> 4/15 </td>
    <td> Balaji Lakshminarayanan </td>
    <td> uncertainty</td>
    <td>
    <ul>
    </ul>
    </td>
    <td></td>
    <td></td>
    </tr>

    <tr>
    <td> 4/22 </td>
    <td> Noah Goodman </td>
    <td> Pyro </td>
    <td>
    <ul>
    </ul>
    </td>
    <td></td>
    <td></td>
    </tr>

    <tr>
    <td> 4/29 </td>
    <td>Poster Presentation</td>
    <td> </td>
    <td>
    <ul>
    </ul>
    </td>
    <td></td>
    <td></td>
    </tr>


</tbody>
</table>


## Course description

In recent years, deep learning has enabled huge progress in many domains
including computer vision, speech, NLP, and robotics. This class is designed to help students develop a
deeper understanding of deep learning and explore new research directions and
applications of AI/deep learning and privacy/security. It assumes that students already have a basic
understanding of deep learning. In particular, in this semester, we will focus on a theme, trustworthy deep learning, exploring a selected list of new, cutting-edge topics including  security and privacy issues in deep learning, explainability, generalization, reliability and robustness, fairness, causality, and theoretical understanding.

## Class format and project

This is a lecture, discussion, and project oriented class. Each lecture will
focus on one of the topics, including a survey of the state-of-the-art in the
area and an in-depth discussion of the topic. Each week, students are expected
to complete reading assignments before class and participate actively in class
discussion.

Students will also form project groups (two to three people per group) and
complete a research-quality class project.

## Enrollment information

**For undergraduates**: Please note that this is a graduate-level class.
However, with instructors' permission, we do allow qualified undergraduate
students to be in the class. If you are an undergraduate student and would like
to enroll in the class, please fill out
[**this form**](https://docs.google.com/forms/d/e/1FAIpQLSdzD9KAcX1oUQ6H1X5LAE_o25umpl6IBrM5LeaSYAvkIuWc8w/viewform?usp=sf_link)
and come to the first lecture of the class. Qualified undergraduates will be
given instructor codes to be allowed to register for the class after the first
lecture of the class, subject to space availability.

If you have not received grades for some classes that you are currently enrolled
in, please choose **Currently Enrolled** and then update the form when you
receive your final grades. You may also be interested in [this
class](https://people.eecs.berkeley.edu/~jfc/DeepLearn.html), which is open to
undergraduates.

Students may enroll in this class for variable units.

* **1 unit:** Participate in reading assignments.
* **2 units:** Both reading assignments and a project. Projects may fall into one of
  four categories:
  * Distill-like Literature Review of a deep learning topic (e.g., a Distill-like blog post illustrating different optimization techniques used in deep learning)
  * Reimplement research code and open source it
  * Conference level research project
* You **may not** take this class for **3 or 4 units**.

## Deadlines

* Reading assignment deadlines:
  * Submit questions about the reading material by Sunday noon.
* Project deadlines:
  * 2/25: Project proposal due
  * 4/1: Project milestone report due
  * 4/29: Poster presentation
  * 5/6: (tentative) Final project report due

## Grading
* 20% class participation
* 25% weekly reading assignment
* 55% project

## Additional Notes
* For students who need computing resources for the class project, we recommend you to look into AWS educate program for students. You'll get 100 dollar's worth of sign up credit. Here's the <a href="https://aws.amazon.com/education/awseducate/apply/"> link </a>.
