---
title: CS294-131 Spring 2019
layout: default
---

## Instructors

<div class="instructor">
  <a href="https://people.eecs.berkeley.edu/~trevor/">
  <div class="instructorphoto"><img src="trevordarrell.jpg"></div>
  <div>Trevor Darrell</div>
  </a>
</div>
<div class="instructor">
  <a href="https://people.eecs.berkeley.edu/~dawnsong/">
  <div class="instructorphoto"><img src="dawnsong.jpg" height="120" width="140"></div>
  <div>Dawn Song
  </div>
  </a>

</div>
<div class="instructor">
  <a href="https://cs.stanford.edu/~jsteinhardt/">
  <div class="instructorphoto"><img src="Jacobsteinhardt.png" height="120" width="140"></div>
  <div>Jacob Steinhardt
  </div>
  </a>

</div>
## Teaching Assistants

<div class="instructor">
  <a href="https://people.eecs.berkeley.edu/~sazadi/">
  <div class="instructorphoto"><img src="Samaneh-Azadi.jpg" height="120" width="140"></div>
  <div>Samaneh Azadi
  </div>
  </a>
</div>

## Office Hours

Samaneh Azadi: By appointment

## Lectures

**Time**: Monday 4:00-5:30pm

**Location**: Soda 306

## Piazza

Course announcements will be announced through Piazza. If you are in the class,
[<a href="https://piazza.com/class/joy4z1cunad9h"> **sign up on Piazza**])(https://piazza.com/class/joy4z1cunad9h).

## Syllabus
<table style="table-layout: fixed; font-size: 88%;">
  <thead>
    <tr>
      <th style="width: 5%;">Date</th>
      <th style="width: 17%;">Speaker</th>
      <th style="width: 10%;">Topic</th>
      <th style="width: 40%;">Readings</th>
      <th style="width: 15%;">Talk</th>
      <th style="width: 15%;">Deadlines</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td> 1/28 </td>
      <td>Jacob Steinhardt</td>
      <td>  course overview/security </td>
      <td>
      <ul>
      </ul>
      </td>
      <td> <li> <a href="https://drive.google.com/file/d/1xquUpTv2tTBMGwrEwz--ok5e4pt4HmJS/view?usp=sharing">Slides</a> </li>
      <li> <a href="https://www.youtube.com/watch?v=H8st9GuC5SI&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8">Video</a> </li></td>
      <td></td>
    </tr>
    <tr>
        <td> 2/4 </td>
        <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#nicolas-paprenot-a-marauder's-map-of-security-and-privacy-in-machine-learning">Nicolas Papernot</a> </td>
        <td>security/privacy </td>
        <td><u>Main Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1705.07204">Ensemble adversarial training</a></li>
        <li><a href="https://arxiv.org/abs/1802.08908">Scalable private learning with PATE</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1703.04730">Understanding black-box predictions via influence functions</a></li>
        <li><a href="https://www.cs.cornell.edu/~shmat/shmat_oak17.pdf">Membership inference attacks against machine learning models</a></li>
        </ul>
        </td>
        <td> <li> <a href="https://drive.google.com/file/d/1yhOrWnM_QJRruDGkodbXmdXMXcsqRrzJ/view?usp=sharing">Slides</a> </li>
        <li> <a href="https://www.youtube.com/watch?v=5GQUXmSg8XU&index=3&t=0s&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8">Video</a> </li></td>
        <td></td>
    </tr>
    <tr>
        <td> 2/11 </td>
        <td> <a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#justin-gilmer-adversarial-examples-are-a-natural-consequence-of-test-error-in-noise">Justin Gilmer </a> </td>
        <td>adversarial examples </td>
        <td><u>Main Reading:</u>
        <ul><li><a href="https://arxiv.org/pdf/1807.01697.pdf">Benchmarking Neural Network Robustness To Common Corruptions And Perturbations</a></li>
        <li><a href="https://arxiv.org/pdf/1901.10513.pdf">Adversarial Examples Are a Natural Consequence of Test Error in Noise</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1807.06732">
        Motivating the Rules of the Game for Adversarial Example Research</a></li>
        </ul>
        </td>
        <td> <li> <a href="https://drive.google.com/file/d/1_XbCeAi7cKVWvTJGFNRrMNcQT819nVpJ/view?usp=sharing">Slides</a> </li>
        <li> <a href="https://www.youtube.com/watch?v=ozt-yu72GpQ&t=0s&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8&index=4">Video</a> </li></td>
        <td></td>
    </tr>
    <tr>
        <td> 2/18 </td>
        <td>Academic Holiday</td>
        <td> - </td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <td> 2/25 </td>
        <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#stefan-wager-deep-learning-for-causal-inference">Stefan Wager</a></td>
        <td>causality </td>
        <td><u>Main Reading:</u>
        <ul><li><a href="http://science.sciencemag.org/content/355/6324/483/tab-pdf">Beyond prediction: Using big data for policy problems</a></li>
        <li><a href="https://arxiv.org/abs/1712.04912">Quasi-oracle estimation of heterogeneous treatment effects</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://www.jstor.org/stable/2289064?seq=1#metadata_info_tab_contents"> Statistics and Causal Inference</a></li>
        <li><a href="http://bayes.cs.ucla.edu/R218-B.pdf"> Causal Diagrams for Empirical Research</a></li>
        </ul>
        </td>
        <td><li> <a href="https://drive.google.com/file/d/1O43Z9Nk-zlBPRl0pvVPzony9_Wwn1-GO/view?usp=sharing">Slides</a> </li></td>
        <td></td>
    </tr>
    <tr>
    <td> 3/4 </td>
    <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#guillaume-bouchard-social-media-platform-safety---ai-against-misinformation">Guillaume Bouchard</a></td>
    <td>fake news defense </td>
    <td><u>Main Reading:</u>
    <ul><li><a href="https://dl.acm.org/citation.cfm?id=3184558.3188723">Relevant Document Discovery for Fact-Checking Articles</a></li>
    <li><a href="https://arxiv.org/pdf/1811.00706.pdf">Combining Similarity Features and Deep Representation Learning for Stance Detection in the Context of Checking Fake News</a></li>
    </ul>
    <u>Background Reading:</u>
    <ul>
    <li><a href="https://arxiv.org/abs/1802.07398"> Investigating Rumor News Using Agreement-Aware Search</a></li>
    <li><a href="http://www.aclweb.org/anthology/W17-1101"> A survey on hate speech detection using natural language processing</a></li>
    </ul>    </td>
    <td></td>
    <td></td>
    </tr>

    <tr>
        <td> 3/11 </td>
        <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#zachary-lipton-deep-learning-under-distribution-shift">Zachary Lipton</a></td>
        <td> explainability</td>
        <td>
        <u>Main Reading:</u>
        <ul><li><a href="https://arxiv.org/abs/1802.03916">Detecting and Correcting for Label Shift with Black Box Predictors</a></li>
        <li><a href="https://128.84.21.199/abs/1903.01689">Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://www.sciencedirect.com/science/article/pii/S0378375800001154"> Improving predictive inference under covariate shift by weighting the log-likelihood function</a></li>
        <li><a href="https://arxiv.org/abs/1505.07818"> Domain-Adversarial Training of Neural Networks</a></li>
        </ul>    
        </td>
        <td><li> <a href="https://www.youtube.com/watch?v=2j549R0PdPA&t=0s&index=5&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8">Video</a> </li></td>
        <td></td>
    </tr>

    <tr>
    <td> 3/18 </td>
    <td> <a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#dustin-tran-the-probabilistic-approach-to-deep-learning">Dustin Tran </a></td>
    <td> Bayesian Deep learning</td>
    <td>
    <u>Main Reading:</u>
    <ul><li><a href="http://dustintran.com/papers/KucukelbirTranRanganathGelmanBlei2017.pdf">Automatic Differentiation Variational Inference</a></li>
    <li><a href="https://arxiv.org/abs/1701.03757">Deep Probabilistic Programming</a></li>
    </ul>
    <u>Background Reading:</u>
    <ul>
    <li><a href="https://arxiv.org/abs/1312.6114"> Auto-Encoding Variational Bayes</a></li>
    <li><a href="http://www.cs.columbia.edu/~blei/papers/Blei2014b.pdf"> Build, Compute, Critique, Repeat: Data Analysis with Latent Variable Models</a></li>
    </ul>  
    </td>
    <td><li> <a href="https://drive.google.com/file/d/1zXaAkhecrdo_WeLaHtgrMJcOz18y6aom/view?usp=sharing">Slides</a> </li>
        <li> <a href="https://www.youtube.com/watch?v=UULqXZN5N6I&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8&index=6&t=0s">Video </a> </li></td>
    <td></td>
    </tr>

    <tr>
        <td> 3/25 </td>
        <td>Spring Break</td>
        <td> -</td>
        <td>
        <ul>
        </ul>
        </td>
        <td></td>
        <td></td>
    </tr>

    <tr>
        <td> 4/1 </td>
        <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#alex-alemi-and-ian-fischer-information-theory-learning-representations-and-robust-generalization">Alex Alemi, Ian Fischer </a></td>
        <td>information theory </td>
        <td>
        <u>Main Reading:</u>
        <ul><li><a href="https://arxiv.org/abs/1807.00906">Uncertainty in the Variational Information Bottleneck</a></li>
        <li><a href="https://openreview.net/pdf?id=rkVOXhAqY7">The Conditional Entropy Bottleneck</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1612.00410">Deep Variational Information Bottleneck </a></li>
        <li><a href="https://arxiv.org/abs/1711.00464"> Fixing a Broken ELBO</a></li>
        <li><a href="https://arxiv.org/abs/physics/0004057">The Information Bottleneck Method</a></li>
        <li><a href="http://www.jmlr.org/papers/volume19/17-646/17-646.pdf"> Emergence of Invariance and Disentanglement in Deep Representations</a></li>
        </ul> 
        </td>
        <td>        
            <li> <a href="https://www.youtube.com/watch?v=7IalLt07bzA&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8&index=7&t=0s">Video </a> </li></td>
        </td>
        <td></td>
    </tr>
    <tr>
        <td> 4/8 </td> 
        <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#been-kim-interpretability---the-myth-questions-and-some-answers">Been Kim</a></td>
        <td> explainability </td>
        <td>
        <u>Main Reading:</u>
        <ul><li><a href="https://arxiv.org/abs/1711.11279">Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</a></li>
        <li><a href="https://arxiv.org/abs/1810.03292">Sanity Checks for Saliency Maps</a></li>
        </ul>
        <u>Background Reading:</u>
        <ul>
        <li><a href="https://arxiv.org/abs/1702.08608">Towards A Rigorous Science of Interpretable Machine Learning </a></li>
        <li><a href="https://arxiv.org/abs/1805.11783"> To Trust Or Not To Trust A Classifier</a></li>
        </ul> 
        </td>
        <td><li> <a href="https://www.youtube.com/watch?v=wzQutYg9KRg&list=PLkFD6_40KJIxG6I7MWd4LXAKl-kQO54_8&index=8&t=20s">Video</a> </li></td>
        <td></td>
    </tr>

    <tr>
    <td> 4/15 </td>
    <td><a href="https://berkeley-deep-learning.github.io/cs294-131-s19/speakers.html#balaji-lakshminarayanan-uncertainty-and-out-of-distribution-robustness-in-deep-learning">Balaji Lakshminarayanan </a></td>
    <td> uncertainty</td>
    <td>
    <u>Main Reading:</u>
    <ul><li><a href="https://arxiv.org/abs/1612.01474">Simple and scalable predictive uncertainty estimation using deep ensembles</a></li>
    <li><a href="https://arxiv.org/abs/1810.09136">Do deep generative models know what they don't know?</a></li>
    </ul>
    <u>Background Reading:</u>
    <ul>
<li><a href="http://mlg.eng.cam.ac.uk/yarin/thesis/1_introduction.pdf">Chapter 1 of Yarin Gal's PhD thesis "Uncertainty in deep learning"
Introduction: The Importance of Knowing What We Donâ€™t Know </a></li>
    <li><a href="https://arxiv.org/pdf/1706.04599.pdf"> On Calibration of Modern Neural Networks</a></li>
    </ul> 
    </td>
    <td></td>
    <td></td>
    </tr>

    <tr>
    <td> 4/22 </td>
    <td> Noah Goodman </td>
    <td> Pyro </td>
    <td>
    <ul>
    </ul>
    </td>
    <td></td>
    <td></td>
    </tr>

    <tr>
    <td> 4/29 </td>
    <td>Poster Presentation</td>
    <td> </td>
    <td>
    <ul>
    </ul>
    </td>
    <td></td>
    <td></td>
    </tr>


</tbody>
</table>


## Course description

In recent years, deep learning has enabled huge progress in many domains
including computer vision, speech, NLP, and robotics. This class is designed to help students develop a
deeper understanding of deep learning and explore new research directions and
applications of AI/deep learning and privacy/security. It assumes that students already have a basic
understanding of deep learning. In particular, in this semester, we will focus on a theme, trustworthy deep learning, exploring a selected list of new, cutting-edge topics including  security and privacy issues in deep learning, explainability, generalization, reliability and robustness, fairness, causality, and theoretical understanding.

## Class format and project

This is a lecture, discussion, and project oriented class. Each lecture will
focus on one of the topics, including a survey of the state-of-the-art in the
area and an in-depth discussion of the topic. Each week, students are expected
to complete reading assignments before class and participate actively in class
discussion.

Students will also form project groups (two to three people per group) and
complete a research-quality class project.

## Enrollment information

**For undergraduates**: Please note that this is a graduate-level class.
However, with instructors' permission, we do allow qualified undergraduate
students to be in the class. If you are an undergraduate student and would like
to enroll in the class, please fill out
[**this form**](https://docs.google.com/forms/d/e/1FAIpQLSdzD9KAcX1oUQ6H1X5LAE_o25umpl6IBrM5LeaSYAvkIuWc8w/viewform?usp=sf_link)
and come to the first lecture of the class. Qualified undergraduates will be
given instructor codes to be allowed to register for the class after the first
lecture of the class, subject to space availability.

If you have not received grades for some classes that you are currently enrolled
in, please choose **Currently Enrolled** and then update the form when you
receive your final grades. You may also be interested in [this
class](https://people.eecs.berkeley.edu/~jfc/DeepLearn.html), which is open to
undergraduates.

Students may enroll in this class for variable units.

* **1 unit:** Participate in reading assignments.
* **2 units:** Both reading assignments and a project. Projects may fall into one of
  four categories:
  * Distill-like Literature Review of a deep learning topic (e.g., a Distill-like blog post illustrating different optimization techniques used in deep learning)
  * Reimplement research code and open source it
  * Conference level research project
* You **may not** take this class for **3 or 4 units**.

## Deadlines

* Reading assignment deadlines:
  * Submit questions about the reading material by Sunday noon.
* Project deadlines:
  * 2/25: Project proposal due
  * 4/1: Project milestone report due
  * 4/29: Poster presentation
  * 5/6: (tentative) Final project report due

## Grading
* 20% class participation
* 25% weekly reading assignment
* 55% project

## Additional Notes
* For students who need computing resources for the class project, we recommend you to look into AWS educate program for students. You'll get 100 dollar's worth of sign up credit. Here's the <a href="https://aws.amazon.com/education/awseducate/apply/"> link </a>.
